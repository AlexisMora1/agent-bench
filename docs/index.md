# 📊 Welcome to the Agent Bench project!

## 🔍 What is Agent Bench?

**Agent Bench** is a simple toolkit that will allow you to perform evaluations of your multi-agent (or single-agent) architectures more easily.

To accomplish this process, this package is developed based on the **GraphEvaluator** tool. With it you will be able to evaluate the ability of one or several architectures to solve any type of task, from one-shift tasks to tasks that can take a full conversation. 

## ⚙️ Why to use Agent Bench?

Agent Bench not only supports the dynamic creation of networks based on **configurations**, but also allows you to streamline the evaluation process through the use of batches, potentially reducing evaluation time. 

It is an ideal tool for edge case testing and data collection, which also allows you to generate a simple report with some default metrics, and even add **evaluators** that allow you to collect and display results for better analysis.

## Project Status 🚨
⚠️ **Limited Support**  
This project is currently in developing mode, and only critical bug fixes will be addressed.  
Currently Agent Bench only supports architectures built using **LangGraph**, and most of the pre-built functions assume that you use **LangChain** tools, although we tried to keep enough flexibility to allow you to use other types of solutions when invoking and managing your agents. 
