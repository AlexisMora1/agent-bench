{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"\ud83d\udcca Welcome to the Agent Bench project!","text":""},{"location":"index.html#what-is-agent-bench","title":"\ud83d\udd0d What is Agent Bench?","text":"<p>Agent Bench is a simple toolkit that will allow you to perform evaluations of your multi-agent (or single-agent) architectures more easily.</p> <p>To accomplish this process, this package is developed based on the GraphEvaluator tool. With it you will be able to evaluate the ability of one or several architectures to solve any type of task, from one-shift tasks to tasks that can take a full conversation. </p>"},{"location":"index.html#why-to-use-agent-bench","title":"\u2699\ufe0f Why to use Agent Bench?","text":"<p>Agent Bench not only supports the dynamic creation of networks based on configurations, but also allows you to streamline the evaluation process through the use of batches, potentially reducing evaluation time. </p> <p>It is an ideal tool for edge case testing and data collection, which also allows you to generate a simple report with some default metrics, and even add evaluators that allow you to collect and display results for better analysis.</p>"},{"location":"index.html#project-status","title":"Project Status \ud83d\udea8","text":"<p>\u26a0\ufe0f Limited Support This project is currently in developing mode, and only critical bug fixes will be addressed. Currently Agent Bench only supports architectures built using LangGraph, and most of the pre-built functions assume that you use LangChain tools, although we tried to keep enough flexibility to allow you to use other types of solutions when invoking and managing your agents. </p>"},{"location":"evaluation_with_conversational_setup.html","title":"\ud83e\uddec Graph Evaluator with Conversational Setup","text":"<p>The Graph Evaluator object is the centre of the whole project, it contains the necessary stuff to:</p> <ol> <li>Dynamically build your graph from configurations</li> <li>Evaluate your configurations by dataset</li> <li>Evaluate your configurations by conversational setup</li> </ol>"},{"location":"evaluation_with_conversational_setup.html#what-means-to-evaluate-an-architecture-by-a-conversational-setup","title":"\ud83d\udd0d What means to evaluate an architecture by a conversational setup?","text":"<p>Some of the times when we build an agent to solve a problem, performing the task may take more than one turn of human-agent interaction to reach the solution, for this kind of tasks we have implemented the evaluate_with_conversational_setup() method.</p> <p>This method allows you not only to run a complete conversation defining the maximum number of turns, but also to customize your type of interaction and evaluation for each setup by implementing an agent_handler().</p>"},{"location":"evaluation_with_conversational_setup.html#conversational-setup","title":"\ud83d\udcda Conversational Setup.","text":"<p>In order to make the evaluation of the graph as flexible as possible, it was decided to use a list of dictionaries in the conversational setup, containing in each key the following important and necessary values</p> <ul> <li><code>iterations</code>: Number of conversation iterations - <code>invoke_state</code>: Initial state for the conversation - <code>preserving_keys</code>: State keys to preserve between turns - <code>conversational_agent</code>: Agent that automatically handles conversation evaluation with your current graph configuration</li> </ul>"},{"location":"evaluation_with_conversational_setup.html#basic-setup","title":"\ud83e\uddf0 Basic Setup","text":"<pre><code>from agent_bench.graph_evaluator import GraphEvaluator\nfrom langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n\n# Define your conversational agent\ndef conversational_agent(messages: List[AnyMessage]):\n    system = SystemMessage(content=\"Your system prompt\")\n    try:\n        response = llm.invoke([system] + messages)\n        messages.append(response)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        messages.append(AIMessage(content=''))\n    return messages\n\n# Define agent setup\nagent_setup = {\n    \"iterations\": 3,  # Number of conversation turns\n    \"invoke_state\": {\n        \"messages\": [HumanMessage(content=\"Initial message\")],\n    },\n    \"preserving_keys\": [\"messages\"],  # State keys to preserve between iterations\n    \"conversational_agent\": conversational_agent\n}\n\n# Initialize evaluator\ngraph_evaluator = GraphEvaluator(\n    configurations=configurations,\n    start_node=\"retrieve\",\n    evaluators=[]\n)\n</code></pre>"},{"location":"evaluation_with_conversational_setup.html#running-evaluation","title":"\u25b6\ufe0f Running Evaluation","text":"<pre><code>results = graph_evaluator.evaluate_with_conversational_config(\n    experiment_name=\"Test1\",\n    graph_state_class=GraphState,\n    agent_handler=AWSProvider.bedrock_agent_handler,  # or your custom handler\n    agent_setup=[agent_setup],\n    batch_size=3\n)\n</code></pre>"},{"location":"evaluation_with_conversational_setup.html#key-components","title":"\ud83d\udddd Key Components","text":""},{"location":"evaluation_with_conversational_setup.html#agent-handler","title":"Agent Handler","text":"<p>An Agent Handler is a function that will allow you to manage the turn-based execution of your architecture.</p> <p>Currently as an example we have bedrock_agent_handler in the integrations module, which will be useful to evaluate architectures that manage the history of the conversation through a list of messages in a key \u201cmessages\u201d.</p>"},{"location":"evaluation_with_conversational_setup.html#evaluators","title":"Evaluators","text":"<p>For conversational evaluation, create evaluators that inherit from BaseEvaluator:</p> <pre><code>class ConversationalEvaluator(BaseEvaluator):\n    def evaluate(self, conversation_history):\n        # Implement conversation evaluation logic\n        return score\n</code></pre>"},{"location":"evaluation_with_conversational_setup.html#output","title":"Output","text":"<p>The evaluation generates: - Conversation metrics - System resource usage - PDF report with evaluation result</p>"},{"location":"evaluators.html","title":"Agent-Bench Evaluators Documentation","text":""},{"location":"evaluators.html#overview","title":"\ud83d\udcdd Overview","text":"<p>Evaluators in Agent-Bench are essential components that measure different aspects of agent performance. The evaluator system is built around the <code>BaseEvaluator</code> class, which provides a flexible framework for creating custom evaluation metrics.</p>"},{"location":"evaluators.html#baseevaluator-class","title":"\ud83c\udfd7 BaseEvaluator Class","text":"<p>The <code>BaseEvaluator</code> class serves as the foundation for all evaluators in the system.</p>"},{"location":"evaluators.html#key-components","title":"\u26d3 Key Components","text":"<pre><code>class BaseEvaluator:\n    def __init__(\n        self,\n        state_key: str = None,\n        aggregation: Callable = None,\n        **kwargs\n    ):\n        self.state_key = state_key\n        self.aggregation = aggregation\n        self.default_plot = kwargs.get(\"default_plot\", False)\n</code></pre> <ul> <li><code>state_key</code>: Specifies which part of the agent's output to evaluate</li> <li><code>aggregation</code>: Function to aggregate multiple evaluation results</li> <li><code>default_plot</code>: Boolean flag to enable automatic plotting of results</li> </ul>"},{"location":"evaluators.html#core-methods","title":"Core Methods","text":"<ol> <li>extract_from_state</li> </ol> <pre><code>def extract_from_state(self, state: Dict):\n    if self.state_key:\n        return state.get(self.state_key, None)\n    return state\n</code></pre> <p>Extracts relevant data from the agent's state using the configured state_key.</p> <ol> <li>evaluate</li> </ol> <pre><code>def evaluate(self, model_output, output_data):\n    raise NotImplementedError\n</code></pre> <p>Abstract method that must be implemented by concrete evaluators.</p>"},{"location":"evaluators.html#pre-built-evaluators","title":"Pre-built Evaluators","text":""},{"location":"evaluators.html#1-similarityevaluator","title":"1. SimilarityEvaluator","text":"<p>Measures semantic similarity between model outputs and expected outputs using sentence embeddings.</p> <pre><code>class SimilarityEvaluator(BaseEvaluator):\n    def __init__(\n        self,\n        state_key,\n        aggregation: Callable,\n        **kwargs\n    ):\n        super().__init__(state_key, aggregation, **kwargs)\n        self.model = SentenceTransformer(\n            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n        )\n</code></pre> <p>\ud83c\udf0c Usage Example:</p> <pre><code>similarity_evaluator = SimilarityEvaluator(\n    state_key=\"generation\",\n    aggregation=np.mean,\n    default_plot=True\n)\n</code></pre> <p>Key Features: - Uses multilingual sentence transformers - Computes cosine similarity between embeddings - Supports automatic plotting of similarity distributions</p>"},{"location":"evaluators.html#2-accuracyevaluator","title":"2. AccuracyEvaluator","text":"<p>Evaluates binary classification accuracy and generates confusion matrices.</p> <pre><code>class AccuracyEvaluator(BaseEvaluator):\n    def __init__(self, state_key):\n        super().__init__(state_key)\n\n</code></pre> <p>\ud83c\udf0c Usage Example:</p> <pre><code>accuracy_evaluator = AccuracyEvaluator(\n    state_key=\"model_response\"\n)\n</code></pre> <p>Key Features: - Binary classification evaluation - Custom confusion matrix plotting - Support for statistical tests</p>"},{"location":"evaluators.html#baseconversationalevaluator","title":"BaseConversationalEvaluator","text":"<p>Extended evaluator class for conversational agents.</p> <pre><code>class BaseConversationalEvaluator(BaseEvaluator):\n    def __init__(\n        self,\n        state_key: str = None,\n        iterations: int = 1,\n        conversational_agent: Callable = None,\n        **kwargs\n    ):\n        super().__init__(state_key, **kwargs)\n        self.iterations = iterations\n        self.conversational_agent = conversational_agent\n</code></pre> <p>Key Features: - Supports multi-turn conversation evaluation - Manages conversation state across iterations - Customizable conversation flow</p>"},{"location":"evaluators.html#creating-custom-evaluators","title":"Creating Custom Evaluators","text":""},{"location":"evaluators.html#basic-template","title":"Basic Template","text":"<pre><code>class CustomEvaluator(BaseEvaluator):\n    def __init__(self, state_key, **kwargs):\n        super().__init__(state_key, **kwargs)\n        # Initialize any required models or resources\n\n    def evaluate(self, model_output, output_data):\n        # Extract relevant data\n        model_data = self.extract_from_state(model_output)\n        expected_data = self.extract_from_state(output_data)\n\n        # Implement evaluation logic\n        return evaluation_score\n\n    def custom_plot(self, dataset: Dict, file_prefix: str):\n        # Optional: Implement custom visualization\n        return [\"path/to/generated/plot.png\"]\n</code></pre>"},{"location":"evaluators.html#best-practices","title":"Best Practices","text":"<ol> <li>State Management</li> <li>Always use <code>extract_from_state</code> to access data</li> <li> <p>Handle missing data gracefully</p> </li> <li> <p>Error Handling</p> </li> <li>Implement robust error handling in evaluate()</li> <li> <p>Return sensible default values on failure</p> </li> <li> <p>Visualization</p> </li> <li>Implement <code>custom_plot</code> for specialized visualizations</li> <li> <p>Use <code>default_plot=True</code> for standard metrics</p> </li> <li> <p>Performance</p> </li> <li>Initialize heavy resources (like models) in init</li> <li>Cache computations when possible</li> </ol>"},{"location":"evaluators.html#integration-with-graphevaluator","title":"Integration with GraphEvaluator","text":"<p>Evaluators are used in GraphEvaluator to assess agent performance:</p> <pre><code>graph_evaluator = GraphEvaluator(\n    configurations=configurations,\n    start_node=\"retrieve\",\n    evaluators=[\n        SimilarityEvaluator(\n            state_key=\"generation\",\n            aggregation=np.mean,\n            default_plot=True\n        ),\n        AccuracyEvaluator(state_key=\"model_response\")\n    ]\n)\n</code></pre> <p>The evaluators will automatically: - Process each agent response - Aggregate results across experiments - Generate visualizations in the final report</p>"},{"location":"getting_started.html","title":"\u2694\ufe0f Getting started","text":"<p>The evaluation of multi- or single-agent architectures is still a complicated issue with most frameworks, and to overcome this repetitive task, this toolkit was developed to offer you the flexibility to adjust to your use case.</p> <p>Before starting the installation make sure you have Python 3.10+ installed, as well as having created your virtual environment.</p>"},{"location":"getting_started.html#instalation","title":"\ud83d\udd28 Instalation","text":""},{"location":"getting_started.html#using-pip","title":"Using pip","text":"<pre><code>pip install git+https://github.com/AlexisMora1/agent-bench.git\n</code></pre> <p>There is no more options to install, thank you!</p>"},{"location":"getting_started.html#key-concepts","title":"\ud83d\udd11 Key Concepts","text":"<p>Before you start using the package we suggest you read this section to get a better understanding of how to best perform your evaluations.</p>"},{"location":"getting_started.html#1-experiments","title":"1. \ud83e\uddea Experiments:","text":"<p>The evaluations in Agent Bench are based on the experiment concept. Each time you run an evaluation you can provide an experiment_id that will be the key to recognize your evaluation. </p> <p>An experiment can have several configurations of architectures and from it the necessary groupings will be made in the report.</p>"},{"location":"getting_started.html#2-configurations","title":"2. \ud83d\udd27 Configurations:","text":"<p>A configuration is a dictionary containing, for each key, another dictionary with the keys \"nodes\" (list of functions representing the LangGraph nodes), \"edges\" (list of tuples representing the connection between nodes if it is a 2-valued tuple, and a conditional edge if it is a 3-valued tuple), and \"eval_config\". </p> <pre><code>configurations = {\n    \"rag\": {\n        \"nodes\": rag_nodes,\n        \"edges\": rag_edges,\n        \"eval_config\": rag_eval_config,\n    },\n    ...\n}\n</code></pre> <p>Sometimes with LangGraph it is complex to construct multiple architectures to test each one, so assuming you have a set of functions representing your nodes and edges, you can define this configuration dictionary to evaluate all configurations together</p>"},{"location":"getting_started.html#3-evaluation-configuration","title":"3. \ud83d\udccb Evaluation Configuration","text":"<p>By modifying the connections between nodes or the logic behind them, you can experiment with the capabilities of the architecture, but sometimes it is also necessary to evaluate the performance of the large language models involved in the agent's decisions.</p> <p>So the evaluation configuration is a dictionary containing the node or edge name and its particular configuration.</p>"},{"location":"getting_started.html#4-evaluators","title":"4. \ud83d\udd2c Evaluators","text":"<p>An Evaluator is the other key part of the Graph Evaluator class. To make a good evaluation most of the time you may need to create your own evaluators that can assign a value to the execution results.</p> <p>When you add an evaluator to an experiment, you can also add the custom_plot() method or the default_plot() attribute to enable the results to be plotted in the final evaluation report.</p>"},{"location":"graph_evaluator_class.html","title":"\ud83e\uddec Graph Evaluator with Evaluation Datasets","text":"<p>The Graph Evaluator object is the centre of the whole project, it contains the necessary stuff to:</p> <ol> <li>Dynamically build your graph from configurations</li> <li>Evaluate your configurations by dataset</li> <li>Evaluate your configurations by conversational setup</li> </ol>"},{"location":"graph_evaluator_class.html#evaluation-dataset","title":"\ud83d\udce6 Evaluation Dataset.","text":"<p>When working with agentic systems, most of the time we need the agent to classify an input into a certain label, and this process could be realised with only one turn of interaction, it means an input is given and we expect the model output to be a defined value.</p> <p>A dataset is an object that contains execution inputs with their corresponding output examples.</p> <p>If your evaluation fits this type of task description, you can pass a dataset when using the evaluate() method and configure a custom evaluator to measure the accuracy of the architecture at this task.</p>"},{"location":"graph_evaluator_class.html#evaluate-method","title":"\ud83d\udce1 evaluate() method","text":"<p>The evaluate() method is one of the main methods for the GraphEvaluator class. </p> <p>It is designed to evaluate a configuration graph in an evaluation dataset and returns an evaluation dataset containing the labels provided before execution, updated with the results and the metrics calculated by the evaluators provided during evaluation.</p>"},{"location":"graph_evaluator_class.html#basic-setup","title":"\ud83e\uddf0 Basic Setup","text":"<pre><code>from agent_bench.graph_evaluator import GraphEvaluator\nfrom agent_bench.evaluators.prebuilt import SimilarityEvaluator\n\n# Initialize evaluators\nsimilarity_evaluator = SimilarityEvaluator(\n    state_key=\"generation\",\n    aggregation=np.mean,\n    default_plot=True\n)\n\n# Define your configurations\nconfigurations = {\n    \"rag\": {\n        ...\n    },\n}\n\n# Initialize the evaluator\ngraph_evaluator = GraphEvaluator(\n    configurations=configurations,\n    start_node=\"retrieve\",\n    evaluators=[similarity_evaluator],\n)\n</code></pre>"},{"location":"graph_evaluator_class.html#dataset-format","title":"\ud83d\udd8b Dataset Format","text":"<p>Your dataset should be a list of dictionaries with this structure:</p> <pre><code>dataset = [\n    {\n        \"input\": {\"key\": \"value\"},\n        \"output\": {\n            \"key1\": \"expected_value1\",\n            \"key2\": \"expected_value2\"\n        }\n    },\n    # More examples...\n]\n</code></pre>"},{"location":"graph_evaluator_class.html#running-evaluation","title":"\u25b6\ufe0f Running Evaluation","text":"<pre><code>results = graph_evaluator.evaluate(\n    dataset=dataset,\n    graph_state_class=GraphState,\n    batch_size=10,  # Optional: for parallel processing\n    experiment_name=\"First Test\"  # Optional: to identify different runs\n)\n</code></pre>"},{"location":"graph_evaluator_class.html#generating-reports","title":"Generating Reports","text":"<pre><code># Generate PDF report with plots and metrics\nresults.generate_report()\n\n# Save results to file\nresults.save_experiment_as(\"results.json\")  # or \"results.csv\"\n</code></pre>"},{"location":"graph_evaluator_class.html#key-parameters","title":"\ud83d\udddd Key Parameters","text":"<ul> <li><code>configurations</code>: Dictionary containing different agent architectures to evaluate</li> <li><code>start_node</code>: Initial node in your graph</li> <li><code>evaluators</code>: List of BaseEvaluator instances</li> <li><code>batch_size</code>: Number of examples to process in parallel</li> <li><code>experiment_name</code>: Identifier for the evaluation run</li> </ul>"},{"location":"graph_evaluator_class.html#output","title":"Output","text":"<p>The evaluation generates: - Execution metrics (time, memory usage, CPU usage) - Custom metrics from evaluators - Plots for metrics with default_plot=True - PDF report with all results</p>"}]}